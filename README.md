# ğŸ§  RAG Crawler â€” Domain-Scoped Retrieval-Augmented Generation Service

## ğŸ“Œ Overview
This project implements a small **Retrieval-Augmented Generation (RAG)** pipeline that:
- Crawls and extracts content from a given website (within the same domain)
- Indexes and embeds the text for similarity search
- Answers user questions **grounded only** in the crawled content with proper **source citations**
- Refuses to answer when evidence is missing (â€œNot found in crawled contentâ€)

The goal is correctness, grounding, and clarity within a limited engineering timebox (4â€“8 hours).

---

## ğŸš€ Architecture

**Pipeline:**  
`crawl â†’ clean â†’ chunk â†’ embed â†’ store â†’ retrieve â†’ generate â†’ answer`

**Components:**
1. **Crawler (`crawler.py`)** â€“ Collects pages within the domain, respects `robots.txt`, and stores text.  
2. **Cleaner (`text_cleaner.py`)** â€“ Extracts main readable content, removing boilerplate (ads, navbars, etc.).  
3. **Indexer (`indexer.py`)** â€“ Splits text into chunks (â‰ˆ800 chars, 200 overlap), embeds using open-source model, and stores in a vector index (FAISS/Chroma).  
4. **Retriever (`retriever.py`)** â€“ Retrieves top-k most relevant chunks for a given question.  
5. **Q&A Service (`qa_service.py`)** â€“ Builds a grounded prompt from retrieved context, queries the model, and returns:
   ```json
   { 
     "answer": "...", 
     "sources": [{ "url": "...", "snippet": "..." }], 
     "timings": { "retrieval_ms": 42, "generation_ms": 311, "total_ms": 353 }
   }
  '''

âš™ï¸ Setup & Installation
git clone https://github.com/<your-username>/rag_crawler.git
cd rag_crawler
python -m venv venv
source venv/bin/activate    # or venv\Scripts\activate on Windows
pip install -r requirements.txt

ğŸ§© Example Usage (CLI)
# Crawl a site (up to 30 pages)
python cli/crawl_cli.py --url https://example.com --max_pages 30

# Index crawled data
python cli/index_cli.py --chunk_size 800 --chunk_overlap 200

# Ask a question
python cli/ask_cli.py "What is the mission of Example Company?"

Example Output

Answerable query

{
  "answer": "Example Companyâ€™s mission is to make web data accessible to everyone.",
  "sources": [
    { "url": "https://example.com/about", "snippet": "Our mission is to make web data accessible..." }
  ],
  "timings": { "retrieval_ms": 51, "generation_ms": 294, "total_ms": 345 }
}


Unanswerable query

{
  "answer": "Not found in crawled content.",
  "sources": [
    { "url": "https://example.com/contact", "snippet": "The website provides contact details..." }
  ]
}

ğŸ“¦ Folder Structure
rag_crawler/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ crawler.py          # Crawl within domain
â”‚   â”œâ”€â”€ text_cleaner.py     # Clean & extract readable text
â”‚   â”œâ”€â”€ indexer.py          # Chunk & embed
â”‚   â”œâ”€â”€ retriever.py        # Retrieve top-k
â”‚   â”œâ”€â”€ qa_service.py       # Grounded QA generation
â”‚   â””â”€â”€ metrics_logger.py   # Latency, errors, stats
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ crawled_pages/
â”‚   â”œâ”€â”€ vector_store/
â”‚   â””â”€â”€ logs/
â”‚
â”œâ”€â”€ cli/
â”‚   â”œâ”€â”€ crawl_cli.py
â”‚   â”œâ”€â”€ index_cli.py
â”‚   â””â”€â”€ ask_cli.py
â”‚
â”œâ”€â”€ tests/
â”œâ”€â”€ docs/
â””â”€â”€ examples/

ğŸ§  Design Choices & Trade-offs

Chunking: 800-character chunks with 200 overlap ensure semantic continuity.

Embedding: sentence-transformers/all-MiniLM-L6-v2 chosen for speed and low resource cost.

Vector Store: FAISS (local, lightweight) for simplicity and speed.

Crawl Limit: Hard cap of 30â€“50 pages to avoid host overload.

Politeness: Crawl delay (1â€“2 s) and respect for robots.txt.

Grounding: Answers strictly limited to retrieved context; no external hallucination.

Refusal Logic: If retrieved context score < threshold â†’ respond â€œNot found in crawled content.â€

Prompt Hardening: Instructs model to ignore any instructions inside pages.

Observability: Logs retrieval & generation times, p50/p95 latencies, and errors.

Extensibility: Can easily swap embedding model or vector backend.

ğŸ§ª Evaluation
Metric	Description	Target
Grounding correctness	Answers must cite valid URLs	âœ…
Retrieval quality (Recall@k)	Top-k context includes relevant info	âœ…
Latency (p95)	< 2 s per query (local)	âœ…
Refusal correctness	No unsupported answers	âœ…

ğŸ”’ Safety & Guardrails

Ignores embedded or injected model instructions in pages.

Restricts all answers to crawled domain.

Declines to answer when evidence insufficient.

No execution of JavaScript or remote scripts.

ğŸ§° Tooling and Prompts

Embedding model: sentence-transformers/all-MiniLM-L6-v2
LLM (open-source): e.g. mistral-7b-instruct, llama-3-8b-instruct, or similar local API
Libraries: requests, beautifulsoup4, trafilatura, faiss-cpu, sentence-transformers, chromadb, fastapi, tqdm, numpy
Prompt Template:

You are a helpful assistant that must answer strictly from the provided context.
If the context does not contain enough information, say exactly:
"Not found in crawled content."

Context:
{retrieved_chunks}

Question: {user_question}
Answer:

ğŸ§¾ Example Evaluation Calls

/crawl â†’ Crawl and store pages

/index â†’ Build embeddings and store vectors

/ask â†’ Retrieve & answer with grounding

Example eval set:

âœ… â€œWhat is Exampleâ€™s mission?â€ â†’ Supported answer

âŒ â€œWho founded Example?â€ â†’ Refusal

ğŸ§­ Next Steps (Future Work)

Improve boilerplate removal using readability-lxml.

Add async crawler for faster coverage.

Integrate reranker model for better top-k relevance.

Containerize with Docker for reproducibility.

Author: Shashank Sagri Nayak

